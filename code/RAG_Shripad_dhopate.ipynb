{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19e7a72-8e07-46c3-96b9-e4acd927f725",
   "metadata": {},
   "source": [
    "### SHRIPAD DHOPATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d7a89-bf0a-42e9-8352-cbf226d1ea5e",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca35261-6ba6-4755-9be8-a89f0093d3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/vlm_upgrade_generativeai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca00d9f-83ed-41d2-aa7d-d7b47f4d7563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb7b0ef-936b-4e81-829d-ad532416726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown function for clean text\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87703278-7bb2-49e7-8c00-250409e47748",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"xxxxxxxxxxxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eddc6ea-7aa8-466c-8f1c-ebf6a18b018b",
   "metadata": {},
   "source": [
    "### Gemini 2.5 flash LLM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1abc4464-dd03-4a88-aca1-8f4d4dfb2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_model(api_key):\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",google_api_key=api_key,\n",
    "                                 temperature=0.2,top_p = 0.95 ,convert_system_message_to_human=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b0eeac-bf7e-41a2-8f0c-7fe4ebebe9c6",
   "metadata": {},
   "source": [
    "### Loading PDF using pdf loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0f18ba-77be-43b6-a703-aee3d3835536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(pdf_path):\n",
    "    pdf_loader = PyPDFLoader(pdf_path)\n",
    "    pages = pdf_loader.load_and_split()\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "888995c6-7940-4d66-bf0b-02c1e1f510bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(pages):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "    context = \"\\n\\n\".join(str(p.page_content) for p in pages)\n",
    "    texts = text_splitter.split_text(context)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e97153-1148-497c-8ed8-ddffc68e60bd",
   "metadata": {},
   "source": [
    "### Using HuggingFace Embedding and FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2263972-6792-420e-9bc8-5b0fff9f1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fe2cb9-2c9f-4d8c-88d6-16fc2dbb16dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b52680f-f5b6-4053-8a74-0d7612b706d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./Samtaai_assignment/data/computer-history.pdf\"\n",
    "model =  load_llm_model(api_key)\n",
    "pages = load_pdf(pdf_path)\n",
    "texts = chunking(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ade04093-d04a-4ebc-b645-dca2deda50ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11878/543635333.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_index = FAISS.from_texts(texts, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "549c2980-0a2f-4a6b-8e92-437becebc1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, model, vector_index, k=3):\n",
    "    template = \"\"\"Use the following context to answer the question in a detailed manner.\n",
    "                If the context does not contain enough information, say \"I don't know.\"\n",
    "                Provide examples or explanations if possible.\n",
    "                \n",
    "                Context:\n",
    "                {context}\n",
    "                \n",
    "                Question:\n",
    "                {question}\n",
    "                \n",
    "                Answer in detail:\n",
    "                \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=model,\n",
    "        retriever=vector_index.as_retriever(search_kwargs={\"k\": k}),\n",
    "        chain_type=\"stuff\",  \n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"query\": question})\n",
    "\n",
    "    return {\n",
    "        \"answer\": result[\"result\"],\n",
    "        \"source_documents\": result.get(\"source_documents\", [])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06490af2-4fbd-4bb2-a8fc-b186dfd42c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anikett/.local/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A microcomputer is a digital computer system that is controlled by a stored program. It utilizes several key components to function:\n",
       "\n",
       "1.  **Microprocessor:** This is a central component that the microcomputer uses.\n",
       "2.  **Programmable Read-Only Memory (ROM):** The ROM's role is to define the instructions that are to be executed by the computer.\n",
       "3.  **Random-Access Memory (RAM):** RAM serves as the functional equivalent of computer memory within the microcomputer system.\n",
       "\n",
       "The production of microcomputers has greatly benefited from silicon chips, which began to be used in 1971."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"Explain microcomputer in detail .\"\n",
    "response = answer_question(user_question, model, vector_index)\n",
    "Markdown(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e34339-2b92-42bd-b588-8c59cae7f1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai_llm)",
   "language": "python",
   "name": "genai_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
