{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "105d7a89-bf0a-42e9-8352-cbf226d1ea5e",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca35261-6ba6-4755-9be8-a89f0093d3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/vlm_upgrade_generativeai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca00d9f-83ed-41d2-aa7d-d7b47f4d7563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb7b0ef-936b-4e81-829d-ad532416726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown function for clean text\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd1ca9ba-a3c5-4292-8874-ab03f462a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"AIzaSyDbk9D7UZ45ME21mvdYGg92yntMfWavZaE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eddc6ea-7aa8-466c-8f1c-ebf6a18b018b",
   "metadata": {},
   "source": [
    "### Gemini 2.5 flash LLM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1abc4464-dd03-4a88-aca1-8f4d4dfb2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",google_api_key=api_key,\n",
    "                             temperature=0.2,top_p = 0.95 ,convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac0d8c8b-3ba5-4e2b-bb83-8d00482b031e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=0.2, top_p=0.95, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7f34e2f4f8e0>, default_metadata=(), convert_system_message_to_human=True, model_kwargs={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b0eeac-bf7e-41a2-8f0c-7fe4ebebe9c6",
   "metadata": {},
   "source": [
    "### Loading PDF using pdf loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a0f18ba-77be-43b6-a703-aee3d3835536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I FLESH:  Before the advent of any kind of computing device at all, hum an beings \n",
      "performed computation by themselves.  This involved the use of fingers, toes and any \n",
      "other part of the body. \n",
      "II WOOD:  Wood becam e a com puting dev ice when it w as first used to design  the \n",
      "abacus.  Shickard in 1621 and Polini in 1709 were both instrum ental to this \n",
      "development. \n",
      "III METALS:  Meta ls we re used in the ea rly m achines of Pascal, Tho mas, and the \n",
      "production versions from firms such as Brundsviga, Monroe, etc \n",
      "IV ELECTROMECHANICAL DEVICES:  As di fferential analyzers, these were present \n",
      "in the early machines of Zuse, Aiken, Stibitz and many others \n",
      "V ELECTRONIC ELEMENTS:  These were used in the Colossus, ABC,  ENIAC, and \n",
      "the stored program computers. \n",
      " \n",
      "This class ification really does no t apply to de velopments in th e last sixty y ears becaus e \n",
      "several kinds of new electro technological devices have been used thereafter. \n",
      " \n",
      "Classification by Capacity \n",
      "Computers can be clas sified accord ing to their capacity.  The term  ‘capacity’ refers to th e \n",
      "volume of work or the data processing capabil ity a computer can handle. Their perform ance \n",
      "is determined by the amount of data that can be stored in memory, speed of internal operation \n",
      "of the computer, num ber and type of periphe ral devices, am ount and type of software \n",
      "available for use with the computer.  \n",
      " \n",
      "The capacity of early generation co mputers was determined by their physical size - the larger \n",
      "the size, th e greater the volum e.  Recent com puter technology however is tending to create \n",
      "smaller machines, making it possible to packag e equivalent speed and capacity in a sm aller \n",
      "format.  Computer capacity is cu rrently measured by the num ber of app lications that it c an \n",
      " 4\n"
     ]
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader(\"computer-history.pdf\")\n",
    "pages = pdf_loader.load_and_split()\n",
    "print(pages[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "888995c6-7940-4d66-bf0b-02c1e1f510bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "context = \"\\n\\n\".join(str(p.page_content) for p in pages)\n",
    "texts = text_splitter.split_text(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e97153-1148-497c-8ed8-ddffc68e60bd",
   "metadata": {},
   "source": [
    "### Using HuggingFace Embedding and FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2263972-6792-420e-9bc8-5b0fff9f1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ade04093-d04a-4ebc-b645-dca2deda50ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11025/543635333.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_index = FAISS.from_texts(texts, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "878a670e-3641-4516-ac04-2e284e1f26ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7f34b57273a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "549c2980-0a2f-4a6b-8e92-437becebc1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following context to answer the question in a detailed manner.\n",
    "If the context does not contain enough information, say \"I don't know.\"\n",
    "Provide examples or explanations if possible.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in detail:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    retriever=vector_index.as_retriever(search_kwargs={\"k\":3}),\n",
    "    chain_type=\"stuff\",  \n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06490af2-4fbd-4bb2-a8fc-b186dfd42c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anikett/.local/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A microcomputer is defined as a **digital computer system** that operates under the control of a **stored program**.\n",
       "\n",
       "Key components that make up a microcomputer include:\n",
       "*   A **microprocessor**\n",
       "*   A **programmable read-only memory (ROM)**\n",
       "*   A **random-access memory (RAM)**\n",
       "\n",
       "Within this system, the **ROM** plays a crucial role by defining the specific instructions that the computer is designed to execute. The **RAM**, on the other hand, serves as the functional equivalent of the computer's main memory, used for temporary data storage and active program execution.\n",
       "\n",
       "The development and production of microcomputers significantly benefited from the advent of **silicon chips**, which began to be widely used starting in 1971."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is MICROCOMPUTERS explain it\"\n",
    "result = qa_chain({\"query\": question})\n",
    "Markdown(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c014dab-5faa-4044-98ab-7253bd3c616f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai_llm)",
   "language": "python",
   "name": "genai_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
